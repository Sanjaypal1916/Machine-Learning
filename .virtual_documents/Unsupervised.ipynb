import matplotlib.pyplot as plt;
import pandas as pd
import numpy as np
from sklearn import linear_model
from sklearn.datasets import load_iris


                                                                        # k-means
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler


iris = load_iris()
dir(iris)
iris.feature_names


df = pd.DataFrame(iris.data, columns=iris.feature_names)
df["target"]= iris.target
df.columns
df = df.drop( columns = ["sepal length (cm)", "sepal width (cm)"])
df.columns


plt.scatter(df["petal length (cm)"], df["petal width (cm)"])


km = KMeans(n_clusters=2)
y_predicts = km.fit_predict(df[["petal length (cm)", "petal width (cm)"]])


df["predict"] = y_predicts


df0 = df[df.predict == 0]
df1 = df[df.predict == 1]


plt.scatter(df0["petal length (cm)"], df0["petal width (cm)"], color = "blue")
plt.scatter(df1["petal length (cm)"], df1["petal width (cm)"],color="red")


scaler = MinMaxScaler()
scaler.fit(df[["petal length (cm)"]])
df["petal length (cm)"] = scaler.transform(df[["petal length (cm)"]])
scaler.fit(df[["petal width (cm)"]])
df["petal width (cm)"] = scaler.transform(df[["petal width (cm)"]])


df


km = KMeans(n_clusters=2)
y_predicts = km.fit_predict(df[["petal length (cm)", "petal width (cm)"]])

df["predict"] = y_predicts

df0 = df[df.predict == 0]
df1 = df[df.predict == 1]

plt.scatter(df0["petal length (cm)"], df0["petal width (cm)"], color = "blue")
plt.scatter(df1["petal length (cm)"], df1["petal width (cm)"],color="red")


                                                                        # naive Bayes


from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()



df = pd.read_csv("titanic.csv")
df.head()


from sklearn.preprocessing import LabelEncoder

X = df[['Sex', "Fare", "Pclass"]]
y = df['Survived']

le = LabelEncoder()
X["Sex"] = le.fit_transform(X["Sex"])
X


from sklearn.model_selection import train_test_split


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
gnb = GaussianNB()
gnb.fit(X_train, y_train)
gnb.predict(X_test)


gnb.score(X_test, y_test)


from sklearn.datasets import load_wine

df = load_wine()
dir(df)


X = df.data
y = df.target


from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2)
mnb = MultinomialNB()
mnb.fit(X_train, y_train)


mnb.predict(X_test)


mnb.score(X_test, y_test)


                                                                # hyperparameter  tunning


from sklearn.model_selection import cross_val_score
                                                # writing kfold method











                                                                # L1 and L2 Regularization


dataset = pd.read_csv("Melbourne_housing_FULL.csv")
dataset.nunique()


cols_to_use = ['Suburb', 'Rooms', 'Type', 'Method', 'SellerG', 'Regionname', 'Propertycount', 
               'Distance', 'CouncilArea', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'Price']
dataset = dataset[cols_to_use]
dataset.head()


dataset.isna().sum()
cols_to_fill_zero = ['Propertycount', 'Distance', 'Bedroom2', 'Bathroom', 'Car']
dataset[cols_to_fill_zero] = dataset[cols_to_fill_zero].fillna(0)

dataset["BuildingArea"] = dataset["BuildingArea"].fillna(dataset.BuildingArea.mean())
dataset["Landsize"] = dataset["Landsize"].fillna(dataset.Landsize.mean())


dataset.dropna(inplace = True)
dataset.isna().sum()


y = dataset["Price"]
X = dataset.drop("Price", axis=1)


X = pd.get_dummies(X, drop_first = True)
X.head()


from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)


from sklearn.linear_model import LinearRegression
lr = LinearRegression().fit(X_train, y_train)                #linearreg
lr.score(X_test, y_test)


from sklearn import linear_model
lasso_reg = linear_model.Lasso(alpha=50, max_iter=100, tol=0.1)    #lasso_reg
lasso_reg.fit(X_train, y_train)
lasso_reg.score(X_test, y_test)



from sklearn import linear_model
lasso_reg = linear_model.Lasso(alpha=50, max_iter=100, tol=0.1)    #lasso_reg
lasso_reg.fit(X_train, y_train)
lasso_reg.score(X_test, y_test)


from sklearn.linear_model import Ridge
ridge_reg= Ridge(alpha=50, max_iter=100, tol=0.1)                 #ridge_reg
ridge_reg.fit(X_train, y_train)
ridge_reg.score(X_test, y_test)





                                                                            #KNN classifications


from sklearn.datasets import load_digits
digit = load_digits()
dir(digit)


X = digit.data
y = digit.target
from sklearn.model_selection import train_test_split


X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3)

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=11)
knn.fit(X_train, y_train)


knn.score(X_test, y_test)


y_pred = knn.predict(X_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
cm


from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))


                                                                # principal component analysis


df = pd.read_csv("heart.csv")
X = df.drop("HeartDisease", axis =1 )
y = df.HeartDisease

X = pd.get_dummies(X, dummy_na=True)


from sklearn.preprocessing import StandardScaler
Scaled = StandardScaler()
xscld = Scaled.fit_transform(X)
xscld


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test=train_test_split(xscld, y, test_size= 0.3 , random_state = 30)


from sklearn.decomposition import PCA
pca = PCA(n_components=13)
Xpca = pca.fit_transform(X)


pca.explained_variance_ratio_


pca.n_components_


X_train_pcs, X_test_pca, y_train_pca, y_test_pca = train_test_split(Xpca, y, test_size=0.3, random_state=30)


from sklearn.svm import SVC 
from sklearn.linear_model import LogisticRegression 
from sklearn.ensemble import RandomForestClassifier 
from sklearn.model_selection import cross_val_score


svc = SVC( C=10, kernel='linear',gamma='auto' )
lr = LogisticRegression(max_iter = 1000)
rfc = RandomForestClassifier(n_estimators=100)


print(cross_val_score(svc, X_train_pcs,y_train_pca ))
print(cross_val_score(lr, X_train_pcs,y_train_pca ))
print(cross_val_score(rfc, X_train_pcs,y_train_pca ))






