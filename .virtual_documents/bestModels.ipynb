                                                                            # kfold


import matplotlib.pyplot as plt;
import pandas as pd
import numpy as np
from sklearn import linear_model
from sklearn.datasets import load_iris


from sklearn.model_selection import StratifiedKFold 
from sklearn.model_selection import KFold
Fold = StratifiedKFold(n_splits=3)
kf = KFold(n_splits=3)

from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression

from sklearn.datasets import load_digits

from sklearn.model_selection import train_test_split


l_model= LogisticRegression(max_iter = 500)
S_model= RandomForestClassifier(n_estimators=100)
R_model= SVC()
digit = load_digits()


X_train, X_test, y_train, y_test= train_test_split(digit.data, digit.target, test_size=0.3)

def get_score(model, X_train, X_test, y_train, y_test ):
    model.fit(X_train,y_train)
    return model.score(X_test, y_test)


for train_index, test_index in kf.split(digit.data, digit.target):
    X_train, X_test, y_train, y_test = digit.data[train_index], digit.data[test_index],\
                                        digit.target[train_index], digit.target[test_index]

    print(get_score(l_model, X_train, X_test, y_train, y_test))
    print(get_score(S_model, X_train, X_test, y_train, y_test))
    print(get_score(R_model, X_train, X_test, y_train, y_test))


from sklearn.model_selection import cross_val_score     #direct method
print(cross_val_score(l_model, digit.data, digit.target))
print(cross_val_score(S_model, digit.data, digit.target ))
print(cross_val_score(R_model, digit.data, digit.target))


                                                                        # hypertunning 


from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC

# print(cross_val_score(SVC(kernel='rbf',gamma='auto', C=10), digit.data, digit.target , cv=5))
# print(cross_val_score(SVC(kernel='linear',gamma='auto', C=10), digit.data, digit.target, cv=5 ))
# print(cross_val_score(SVC(kernel='poly',gamma='auto', C=10), digit.data, digit.target, cv=5 ))
# print(cross_val_score(SVC(kernel='rbf',gamma='scale', C=10), digit.data, digit.target, cv=5 ))
# print(cross_val_score(SVC(kernel='rbf',gamma='auto', C=50), digit.data, digit.target , cv=5))
# print(cross_val_score(SVC(kernel='rbf',gamma='auto', C=100), digit.data, digit.target , cv=5))

# INSTEAD

kernel = ["rbf", "linear", "poly", "sigmoid"]
C = [10,20,30,40,50,60,70,80,90,100]
average_score = {}

for i in kernel:
    for j in C:
        k = cross_val_score(SVC(kernel=i, gamma="scale", C=j), digit.data, digit.target,  cv=5)
        average_score[i+"_"+str(j)] = np.average(k)

average_score






# instead use gridSearchCV

from sklearn.model_selection import GridSearchCV
gscv = GridSearchCV(SVC(gamma="auto"),{
    "C":[1,10,50,100],
    "kernel": ["linear", "rbf"]
}, cv=5 )

gscv.fit(digit.data, digit.target)
print(gscv.cv_results_)


print(gscv.best_params_)
print(gscv.best_score_)


df = pd.DataFrame(gscv.cv_results_)
df


k = df[["param_C", "param_kernel", "mean_test_score"]]
k


                                                # randomisedCV
from sklearn.model_selection import RandomizedSearchCV
gscv = RandomizedSearchCV(SVC(gamma="auto"),{
    "C":[1,10,50,100],
    "kernel": ["linear", "rbf"]
}, cv=5, 
    return_train_score=False,
    n_iter=3)

gscv.fit(digit.data, digit.target)
gscv.cv_results_

print(gscv.best_params_)
print(gscv.best_score_)






