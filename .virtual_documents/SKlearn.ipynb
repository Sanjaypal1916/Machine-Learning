import matplotlib.pyplot as plt;
import pandas as pd
import numpy as np
from sklearn import linear_model


                                                            # simple linear Regression


df = pd.read_excel("Book1.xlsx")
print(df)



plt.scatter(df.area , df.price, color= "red", marker= "+")
plt.xlabel("area(sq)")
plt.ylabel("price($)")



reg = linear_model.LinearRegression()
reg.fit(df[['area']],df.price)



reg.predict([[33000]])


x =reg.coef_


y =reg.intercept_


z = x*33000 + y
print(z)


                                                                        # practice


df = pd.read_csv("canada_per_capita_income.csv")
print(df.columns)

plt.scatter(df["year"], df["per capita income (US$)"], marker= "+", color="red")



reg.fit(df[["year"]], df["per capita income (US$)"])
reg.predict([[2020]])


print(reg.coef_)
print(reg.intercept_)
print(2020 * reg.coef_ +reg.intercept_)


                                                                    # multi-linear regression models


df = pd.read_excel("book2.xlsx")
print(df)


average = df.bedroom.median()

import math
a = math.floor(average)
df.bedroom = df.bedroom.fillna(a)
df.bedroom


reg = linear_model.LinearRegression()
reg.fit(df[["area","bedroom","age"]],df.price )
reg.predict([[3400, 4, 5]])


print(reg.coef_)
print(reg.intercept_)


-3.35156467e+00 * 3400 + 2.00909984e+04 * 4 + -5.73434706e+02 * 5 + 21135.11286915667


df = pd.read_excel("Book2-2.xlsx")
df
p = reg.predict(df)

df["price"] = p;
print(p)
df.to_excel("book2-2.xlsx", index=False)


                                                                 # saving the model into a file via "Pickle" & "Joblib"


import pickle 
with open ("model_name", "wb") as f:
    pickle.dump(reg, f)


with open ("model_name", "rb") as f:
    m = pickle.load(f)

m.predict([[3400, 4, 5]])


                                                                                # joblib


import joblib

joblib.dump(reg, "model_joblib")

k = joblib.load("model_joblib")
k.predict([[3400, 4, 5]])


                                                                            # get_dummies 


df = pd.read_csv("carprices.csv")


dummy = pd.get_dummies(df["Car Model"],dtype=int)


final = pd.concat([df, dummy], axis=1)
final =final.drop(["Car Model","Mercedez Benz C class"], axis=1)


x = final.drop(["Sell Price($)"], axis = 1)
y = final["Sell Price($)"]


from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x,y)
model.predict([[60000, 12, 0,0]])


                                                                                #OneHotEncoding


from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
dfle = df
dfle["Car Model"] = le.fit_transform(dfle["Car Model"])
dfle


x = dfle[["Car Model","Mileage", "Age(yrs)" ]].values
y = dfle["Sell Price($)"].values
x


from sklearn.preprocessing import OneHotEncoder                                #deprecated
ohe = OneHotEncoder(categorical_features = [0])
f = ohe.fit_transform(x).toarray()



from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
                                                                                # OG
ohe = ColumnTransformer(
    transformers=[('onehot', OneHotEncoder(), [0]) ],
    remainder='passthrough'  
)
f = column_transformer.fit_transform(x)
x = f[:, 1:]
x


model.fit(x,y)
model.predict([[1,0,69000,6]])


model.score(x, y)


                                                            # train_test_split method
                                                        


df = pd.read_excel("book2.xlsx")                             #only used for three dim
df
y = df["price"]
x = df[['area', 'bedroom', 'age']]

plt.scatter( df["age"], df["price"])


from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)


print(len(x_train))


from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x_train, y_train)


model.predict(x_test)


y_test


model.score(x_test, y_test)


                                                              #logistic Regression for binary classification



df = pd.read_csv("HR_comma_sep.csv")
df.head(5)



# employeee salary on retention
le = df[df.left == 1]
print(le.shape)
re = df[df.left == 0]
print(re.shape)


k = df.drop(["Department", "salary"], axis=1)
k = k.groupby("left").mean()
k


pd.crosstab(df.salary, df.left).plot(kind="bar")


from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df[["satisfaction_level","last_evaluation", "number_project", "average_montly_hours", "time_spend_company", "Work_accident", "promotion_last_5years"]], df["left"], test_size=0.1)


model = LogisticRegression()




model.fit(X_train, y_train) 


model.predict(X_test)


model.score(X_test, y_test)


                                                            # logistic regression for multiple classification


from sklearn.datasets import load_iris
%matplotlib inline

container = load_iris()




from sklearn.model_selection import train_test_split

X= container.data
y=container.target
X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2)


from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train,y_train)



y_predict= model.predict(X_test)


model.score(X_test, y_test)


                                                                                # confusion metrics


from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test,y_predict )
cm


import seaborn as sn
plt.figure(figsize = (2,1.5))
sn.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')
plt.show()





df = pd.read_csv("titanic.csv")
X = df[["Pclass","Sex", "Age","Fare"]]
y = df["Survived"]

from sklearn.preprocessing import LabelEncoder 
le_sex = LabelEncoder()





X["Sex"] = le_sex.fit_transform(X["Sex"])


k = X["Age"].mean()
X["Age"] = X["Age"].fillna(k)



X


from sklearn import tree
model =  tree.DecisionTreeClassifier()
model.fit(X, y)


model.predict([[1, 1, 29, 53.100]] )


model.score(X,y)


                                                                        # Support vector machine


from sklearn.datasets import load_digits

digits = load_digits()
dir(digits)



plt.matshow(digits.images[100])
digits.target



df = pd.DataFrame(digits.data, columns=digits.feature_names)
df["target"] = digits.target
df


# df0= df
# df0["target"]= df.target.apply(lambda x : digits.target_names[0])
# df1 = df.target.apply(lambda x : digits.target_names[1])


df


X = df.drop("target", axis = 1)
y = df[["target"]]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test =train_test_split(X,y,test_size= 0.2)


from sklearn.svm import SVC

model = SVC()
model.fit(X_train,y_train)


y_predict = model.predict(X_test)


model.score(X_test, y_test)


from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_predict)
cm


sn.heatmap(cm, annot=true)



